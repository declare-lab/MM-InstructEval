Model,ScienceQA,PuzzleVQA,MMMU,MVSA-Single,MVSA-Multiple,TumEmo,MOSI-2,MOSI-7,MOSEI-2,MOSEI-7,Twitter-2015,Twitter-2017,MASAD,Hate,Sarcasm,MNRE,Wins1, Wins3
[ChatGPT](https://openai.com/blog/chatgpt),61.41,-,-, 2.21, 1.33, 33.34, 37.53, 58.55, 34.77, 74.52, 82.43, 52.06, 45.26, 28.13, 30.68, 79.58,-,-
[LLaMA1-7B-hf](https://huggingface.co/baffo32/decapoda-research-llama-7B-hf),-48.40,-,-, -48.88, -48.63, -37.10, -25.66, -28.66, -28.45, -53.58, -44.28, -46.89, -46.30, -17.31, -26.08, -95.75,-,-
[LLaMA1-13B-hf](https://huggingface.co/srikanthmalla/decapoda-research-llama-13b-hf/tree/main),-51.18,-,-, -18.16, -12.29, -39.41, -24.07, -25.46, -14.96, -42.92, -25.19, -18.97, -16.05, -23.89, -35.26, -75.26,-,-
[LLaMA2-7B-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf),-35.38,-,-, -6.07, -2.89, -26.21, -33.90, -46.06, -11.52, -57.28, -43.07, -37.23, -36.11, -33.83, -22.16, -92.47,-,-
[LLaMA2-13B-hf](https://huggingface.co/meta-llama/Llama-2-13b-hf),-47.46,-,-, -23.44, -23.19, -17.15, -25.69, -44.34, -29.69, -61.47, -40.84, -39.93, -44.08, -35.05, -32.39, -82.78,-,-
[LLaMA3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct),-7.66,-,-, 8.39, 7.63, -2.91, 12.07, -6.98, 2.63, -14.61, 0.28, 6.81, 21.30, 15.18, 2.52, 97.95,-,-
[Mixtral-AWQ](https://huggingface.co/ybelkada/Mixtral-8x7B-Instruct-v0.1-AWQ),34.42,-,-, 3.88, 0.19, 16.84, 6.79, 27.87, 1.14, 56.47, 7.64, 8.89, -0.00, 18.73, 25.29, 50.85,-,-
[Gemma-7B](https://huggingface.co/google/gemma-7b-it),20.33,-,-, 47.36, 38.92, 25.60, 14.90, 13.58, 2.21, -13.75, 12.68, 22.13, 23.09, 17.54, 7.43, 35.41,-,-
[Flan-T5-xxl](https://huggingface.co/google/flan-t5-xxl),73.93,-,-, 34.72, 38.92, 47.00, 38.02, 51.50, 43.86, 112.63, 50.35, 53.13, 52.89, 30.50, 49.99, 82.47,-,-
[Gemini-V](https://ai.google.dev/?gad_source=1&gclid=CjwKCAiA6KWvBhAREiwAFPZM7nRlqCw_2aeZ4cxrQgLHumHdCWiVqSk73-kR7bRJXcxWxlGGt-r1WxoCCjEQAvD_BwE),29.1, -20.14, 14.17, 40.74, 10.52, 29.98, 24.2, 45.47, 50.42, -3.2, -3.42, 21.42, 34.58, 66.87, 22.32, 94.82, 1, 3
[OpenFlamingo](https://huggingface.co/openflamingo/OpenFlamingo-9B-deprecated),-66.71, -53.38, -57.18, -51.64, -58.45, -51.11, -38.71, -63.69, -36.5, -78.28, -63.44, -57.15, -51.35, -33.7, -36.56, -95.11,0,0
[Fromage](https://github.com/chiayewken/multimodal-inference/releases/download/v0.1.0/fromage_model.zip),-75.47, -72.43, -68.42, -74.13, -88.49, -81.48, -70.79, -84.53, -71.88, -92.83, -87.2, -82.27, -75.26, -52.95, -45.58, -99.66,0,0
[LLaVA-v0-7B](https://huggingface.co/liuhaotian/LLaVA-7b-delta-v0),-29.85, -46.48, -41.62, 20.76, -12.51, -45.06, -6.98, -13.52, 10.12, -52.97, -45.3, -24.01, 3.93, -49.1, -17.35, -89.4,0,0
[LLaVA-v0-13B](https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0),-25.15, -67.29, -65.52, 33.36, 1.49, -7.22, -6.94, -26.41, 13.22, -40.87, -31.14, -5.83, 27.92, -24.21, -15.23, -87.26,0,0
[LLaVA-v1.6-7B](https://huggingface.co/liuhaotian/llava-v1.6-vicuna-7b),5.38, -17.94, -12.19, 12.46, 0.29, 20.65, 19.31, -8.63, 41.41, -47.1, -3.13, 10.89, 28.34, 52.32, 45.68, -54.26,0,0
[LLaVA-v1.6-13B](https://huggingface.co/liuhaotian/llava-v1.6-vicuna-13b),14.82, -20.13, 2.57, 19.99, -8.33, 43.62, 25.98, 19.65, 29.76, 10.89, 4.41, 16.59, 29.3, 59.82, 54.56, -15.91, 0,3
[MiniGPT4](https://drive.google.com/file/d/1a4zLvaiDBr-36pasffmgpvH5P7CKmpze/view),-11.39, -29.48, -22.4, 41.99, -2.92, 42.23, 8.69, 6.28, 33.21, -24.96, -23.26, 1.55, 24.31, 41.81, 28.09, -92.14,0,1
[mPLUG-Owl](https://huggingface.co/MAGAer13/mplug-owl-llama-7b), -51.63, -39.29, -43.82, -17.88, -43.61, -31.29, -33.59, -39.34, -27.84, -66.64, -56.56, -47.32, -31.18, 9.72, -4.55, -86.22,0,0
[mPLUG-Owl2.1](https://huggingface.co/Mizukiluke/mplug_owl_2_1),4.02, -24.51, -13.88, -5.57, -9.34, 34.21, 15.04, 14.65, 22.82, -13.04, 1.67, 9.54, 20.66, 42.36, 40.18, -36.67,0,0
[LLaMA-AdapterV2](https://github.com/OpenGVLab/LLaMA-Adapter),-6.59, -46.57, -55.69, 41.21, 11.45, 0.09, -8.55, -9.48, 20.2, -51.28, -34.77, -8.92, 10.58, 4.85, 16.99, -85.2,0,0
[VPGTrans](https://github.com/VPGTrans/VPGTrans?tab=readme-ov-file),-43.92, -62.55, -53.01, 12.25, -13.51, 8.77, -20.5, -33.39, -4.39, -47.0, -47.45, -38.11, -14.56, -23.58, 24.73, -95.06,0,0
[Multimodal-GPT](https://github.com/open-mmlab/Multimodal-GPT),-68.31, -71.87, -65.42, -52.14, -57.55, -51.68, -48.83, -55.39, -45.53, -82.97, -56.97, -52.24, -38.9, -18.68, -21.09, -95.15,0,0
[LaVIN-7B](https://github.com/luogen1996/LaVIN),-26.6, -55.69, -57.57, -43.74, -54.21, -54.99, -32.75, -46.14, -36.92, -68.54, -61.57, -57.54, -32.05, 7.0, 27.58, -87.28,0,0
[LaVIN-13B](https://github.com/luogen1996/LaVIN),-23.1, -41.64, -51.6, -32.3, -57.07, -31.38, -14.52, -33.05, -9.08, -46.96, -57.0, -45.79, -21.4, -4.87, -9.61, -74.77,0,0
[Lynx](https://github.com/bytedance/lynx-llm/tree/main),-61.34, -68.47, -71.54, -0.42, -16.14, -22.35, -31.13, -67.05, -14.45, -81.81, -34.28, -17.5, 0.44, -9.58, -27.47, -80.63,0,0
[Fuyu-8B](https://huggingface.co/adept/fuyu-8b),-38.57, -28.36, -39.52, -51.43, -47.75, 16.44, 6.51, -42.37, 12.75, 4.92, 12.95, 1.9, -22.48, 39.66, 36.38, -97.33,0,0
[LaVIT](https://huggingface.co/rain1011/LaVIT-7B-v2),-35.15, -33.32, -44.78, 5.81, -12.38, -9.29, -12.69, -17.83, -2.38, -63.04, -37.07, -13.85, -7.72, 21.83, 5.26, -98.99,0,0
[LLaMA-3.2-11B-Vision](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct),17.78, 61.75, 54.9, -5.58, 27.47, -11.14, 4.0, -0.5, -13.49, 59.37, 51.33, 12.86, -17.12, -30.99, -44.78, 132.87,1,1
[MiniCPM-V2.6](https://huggingface.co/openbmb/MiniCPM-V-2_6),65.91, 108.09, 95.1, 11.02, 50.05, 24.88, 20.67, 48.38, -25.38, 63.65, 60.17, 40.08, 10.27, -3.69, -31.43, 112.6,0,6
[BLIP-2](https://huggingface.co/Salesforce/blip2-flan-t5-xxl),33.45, -20.81, -4.4, 33.24, 7.23, 48.37, 32.1, 44.64, 55.24, 36.48, 5.64, 27.44, 33.17, 53.54, 83.36, 67.07,2,5
[InstructBLIP](https://huggingface.co/Salesforce/instructblip-flan-t5-xxl),37.34, -21.23, -8.8, 42.63, 12.41, 55.9, 31.24, 38.49, 53.27, 33.86, 2.3, 27.47, 37.1, 54.82, 86.55, 75.62,4,7
[GLM4V-9B](https://huggingface.co/THUDM/glm-4v-9b),72.26, 105.34, 95.99, 4.41, 43.21, -4.55, 13.65, 39.65, -21.05, 62.74, 64.72, 36.75, -10.5, -22.38, -25.3, 122.47,2,4
[InternVL2.5-8B](https://huggingface.co/OpenGVLab/InternVL2_5-8B),62.12, 95.95, 80.16, 2.47, 44.4, 17.59, 14.42, 46.45, -0.46, 83.17, 59.67, 38.17, 14.49, 6.08, -32.77, 111.1,0,3
[DeepSeek-VL2-tiny](https://huggingface.co/deepseek-ai/deepseek-vl2-tiny),22.98, 113.17, 99.4, -8.99, 23.03, -4.78, 17.68, -28.24, -32.96, 48.85, 52.23, 13.51, 3.69, -93.03, -35.12, 63.04,2,2
[DeepSeek-VL2-small](https://huggingface.co/deepseek-ai/deepseek-vl2-small),20.95, 48.05, 40.61, -21.39, 25.14, -32.48, -0.43, 23.19, -21.5, 42.03, 44.4, 14.35, -0.74, -93.87, -44.01, 36.85,0,0
[DeepSeek-VL2](https://huggingface.co/deepseek-ai/deepseek-vl2),5.64, 93.44, 67.98, -2.47, 42.39, 7.92, 17.57, 16.49, -22.74, 57.75, 40.14, 19.65, -0.81, -32.36, -34.57, 78.2,0,0
[Qwen-VL-Chat](https://huggingface.co/Qwen/Qwen-VL-Chat),2.5, -27.45, -9.92, 16.52, -0.21, 38.59, 5.99, 5.2, 9.87, 15.57, 14.69, 23.83, 15.74, 37.05, 53.65, -21.78,0,0
[Qwen2-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct),45.94, 56.13, 61.85, 7.08, 40.2, -3.61, 14.69, 56.41, 4.45, 77.74, 59.6, 44.92, 8.53, -12.44, -25.4, 132.87,2,3
[Qwen2-VL-72B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct),27.47, 51.71, 51.85, 5.39, 50.2, 42.12, 18.23, 68.24, 16.94, 95.6, 42.69, 34.13, 26.97, 35.99, -26.68, 120.77,3,3
[Qwen2.5-VL-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct),39.48, 53.59, 42.88, 6.55, 44.73, 11.91, 10.11, 48.67, 4.13, 80.97, 61.76, 23.03, -9.13, -36.63, -21.74, 118.28,0,2
[Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct),56.65, 81.83, 79.81, 9.79, 48.28, -0.85, 26.35, 47.72, 8.72, 87.9, 64.21, 32.46, 3.18, 8.32, -26.08, 126.24,0,5


